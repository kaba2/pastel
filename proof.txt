Proofs about everything
=======================

### Distance from the mean point to the vertices

Let ''P'' be regular. Define:

''c = 1 / (n + 1) sum_{i = 0}^n p_i''

Then it can be shown that:

''\exists g in RR: \forall i in [0, n]: |p_i - c| = g_n''

Furthermore, it can be shown that ''g_n'' is given by:

''g_n = d sqrt(1 - (3 / 4)^n)''

### Generation of a regular simplex

In the following we describe a procedure to generate
a set of vertices ''V = {v_0, ..., v_n} sub RR^n'' such that
for all ''m in [0, n]'' ''V_m = {v_0, ..., v_m} sub V'' is a
regular m-simplex. The procedure is as follows:

''v_0 = 0''

''v_i = 1 / i sum_{k = 0}^{i - 1} v_j + w_i e_i''

where ''w_i in RR'' and ''e_i in RR^n'' is the i:th standard basis axis. 
Here ''w_i'' must be chosen such that ''|v_i - v_0| = |v_i| = d''.
It can be shown that they are given by:

''w_i = sqrt(d^2 - g_{i - 1}^2) = d (3/4)^{(i - 1) / 2}''

Thus:

''v_i = 1 / i sum_{k = 0}^{i - 1} v_j + d (3/4)^{(i - 1) / 2} e_i''

For example, in ''RR^4'':

''v_0 = d [0, 0, 0, 0]^T''

''v_1 = d [1, 0, 0, 0]^T''

''v_2 = d [1 / 2, sqrt(3) / 4, 0, 0]^T''

''v_3 = d [1 / 2, sqrt(3) / 12, 3 / 4, 0]^T''

''v_4 = d [1 / 2, sqrt(3) / 12, 3 / 16, (3 * sqrt(3)) / 8]^T''

### Simplicial tesselation of ''RR^n''

Partition ''RR^n'' with a set of almost-disjoint (intersection set
has measure zero) regular simplices of edge length ''d'', such that
one of the simplices is given by the generation procedure above.
Then we can take think of the vertices of the simplices as a linear
transform of the integer grid. This linear transformation is given by:

''M = [v_1 - v_0, ..., v_n - v_0] = [v_1, ..., v_n]''.

Notice that ''M'' is lower-triangular and therefore admits fast
solving of linear equations ''Mx = b''.

Theory of views
===============

Given is a class C. We need to give the user 
a way to inspect and modify internal structure, 
but only in a manner which guarantees that no
class invariants are broken.

This does not mean that the user can't be allowed
direct references to an object's inner data.
Actually, more than often with basic data structures
this is essential to achieve good performance.
As an example, think of array objects, such as std::vector.
However, what the direct reference should not
make possible is the ability to break class invariants.

A user of an object is most often interested in
more than one part of the data structure. In addition,
several different users can be interested in
the same data structure. This means that in however
the inner structure of an object is referred to, it
must be possible to create somekind of an
object to represent that reference.
Iterators are an example of such classes of objects.
However, they are very specific such objects, because
the concept of iterator usually implies the traversal of a 
linear sequence, per how this concept is used in the
C++ standard library. Iterators also include
a possibility to move along the linear structure.

A view is a concept that embodies only the ability
to refer to some subpart of an object and then
some set of inspection and modification functions.
A view is always specified by the class C.
Iterators are a refinement of a view. These
views are usually quite simple since the iterators
always refer to a single element of a sequential container.
However, let us now concentrate on the implementation
of a 2-dimensional array class Array. What kind of views
should Array offer? The simplest one is arguably
to offer a view for a single element specified
by a 2-d coordinate. Here it would make sense
to offer a direct reference to the array element.
Another view could be given by considering the
array as a sequential 1-d array when considering
the 2d array elements in row-major order. Here
we would offer iterators to pass through the
elements of an array. However, iterators are
again an example of a trivial view.

To give an example of a more complex view,
consider we offer an array view to all of the
data. This view offers an interface very
similar to the array itself. However, the
difference is that it omits any ways which
could modify the essential state of an 
Array object. By essential state we mean
the width and height of the array, as
well as the pointer that points to the
dynamically allocated memory for the data.
By inessential state we mean that data
whose modification does not allow to
break the invariants of the class.
_Views are devices to affect the inessential
state of an object_.

Of course, our view point is on the side
of the class. What is inessential state for
the class is essential state for the user,
and what is essential state for the class
is inessential state for the user.

A common mistake in object oriented programming
is to create a class for a data structure and
then implement algorithms that use that 
data structure as its member functions. Most often,
the set of possible algorithms for using
the data structure are unbounded, and this
leads to the class interface changing
constantly. Probably the number one cause
this is so common is that the programmer concludes
that there is no way to implement the 
algorithm outside the class since there
is no way to refer to the internal data structures
of the class. Here the wrong conclusion is to
implement the algorithm as a member function using 
implementation details of the data structure.
This results in massive classes which contains
algorithms not necessarily so strongly related
to either the data structure or each others.
What this situation is actually manifesting is
that the author of the class hasn't provided
proper views to the internal data to actually
be able to use the data structure _from the outside_.
The number two cause for this mistake is that
while the class actually offers the needed views,
the programmer still somehow feels that it is part of the
object oriented way to include everything as a 
member function of some class. A specific example is given
by a square matrix class for which one implements
a member function to compute the determinant,
while the function could as well be implemented
outside the class. This is unfortunate for the following
reasons:

1) The class interface is never completed. I.e.
it changes everytime a new algorithm needs to be
implemented.
2) The algorithms are not encapsulated from the
changes to the internal workings of the class.
An innocuous change can mean rewriting all
of the algorithms.
3) There is no natural class for placing
a function which involves two classes
with equal importance.
4) Those users that do not need the new algorithms
get them anyway, resulting in increased
physical dependencies and thus increased
compilation and link times.

Number of degrees of freedom for a rotation
===========================================

We shall abbreviate degrees of freedom
by dof. A rotation in R^n is a linear transformation.
Linear transformations in R^n can be identified
with n x n matrices and thus they have n^2 dofs.

An orthogonal matrix is constrained such that 
its columns must be of unit length (n constraints)
and the columns must be orthogonal to each other 
((n choose 2) constraints). Thus the number of dofs
available for a rotation is:

dofs 
= n^2 - n - (n choose 2)
= n^2 - n - n(n - 1) / 2
= (2n^2 - 2n - n^2 + n) / 2
= (n^2 - n) / 2
= n(n - 1) / 2

For example:
n	dofs
1	0
2	1
3	3
4	6
5	10

Least squares rigid body transformation
=======================================

f(x) = Qx + r

E = sum_i E_i

E_i = |Qx_i + r - y_i|^2

= (Qx_i + r - y_i)^T (Qx_i + r - y_i)

= x_i^T Q^T Q x_i + r^T Q x_i - y_i^T Q x_i +
x_i^T Q^T r + r^T r - y_i^T r -
x_i^T Q^T y_i - r^T y_i + y_i^T y_i

= x_i^T x_i + 2r^T Q x_i - 2r^T y_i + y_i^T y_i +
2 x_i^T Q^T y_i + r^T r

Dr(E_i) = 2 x_i^T Q - 2 y_i^T + 2 r^T = 0

=>

y~ = Q x~ + r

Where x~ denotes the mean of {x_i}.
Thus the mean of {x_i} maps to the mean of {y_i}.
This way we eliminate the translation term r:

r = y~ - Q x~

Now we just have to find Q.

Volume of a simplex
===================

Problem
-------

An n-simplex S is given as a set:
S = {x in R^n | sum(x) <= 1, allGreaterEqual(x, 0)}
Compute the volume of S.

Solution
--------

volume(S) 
= V  
= int[S] 1 dS
=
int[0..1]
int[0..(1 - u_1)]
int[0..(1 - (u_1 + u_2))]
...
int[0..(1 - sum[i = 1..n-1](u_i))]
1
du_n
du_(n-1)
...
du_1

We will show by induction that the integrand at the k:th integration is:
I_k(u_1, ..., u_(n-(k-1)))
= (1 / (k - 1)!) (1 - sum[i = 1..(n-(k-1))](u_i))^(k-1)

First note that the formula holds for k = 1:
I_1(u_1, ..., u_n) = 1

Now assume that the formula holds for 1 < k <= n. We will show that
the formula holds for k+1.

int[0..(1 - sum[i = 1..n-k](u_i))] I_k(u_1, ..., u_(n-(k-1)) du_(n-(k-1))
= int[0..(1 - sum[i = 1..n-k](u_i))] (1 / (k - 1)!) (1 - sum[i = 1..(n-(k-1))](u_i))^(k-1) du_(n-(k-1))
= (-(1 / (k - 1)!) / k) [0..(1 - sum[i = 1..n-k](u_i))] ((1 - sum[i = 1..(n-k)](u_i)) - u_(n-(k-1)))^k
= (-(1 / k!)) [0 - (1 - sum[i = 1..(n-k)](u_i))^k]
= (1 / k!) (1 - sum[i = 1..(n-k)](u_i))^k
= I_(k+1)(u_1, ..., u_(n-k))

Thus:

V = I_(n+1) = 1 / n!

Bumpy cubic
===========

Problem
-------

f(x) = ax^3 + bx^2 + cx + d
f'(x) = 3ax^2 + 2bx + c

Solve a, b, c, d from
f(0) = H
f(R) = 0
f'(0) = 0
f'(R) = 0

Solution
--------

f(0) = d = H
f'(0) = c = 0

f'(R) = 3aR^2 + 2bR = 0
=>
b = -3aR^2 / 2R = (-3/2)aR

f(R) = aR^3 + bR^2 + H
= aR^3 + (-3/2) aR R^2 + H
= (-1/2) aR^3 + H
= 0

=>
a = 2H / R^3

=>
b = (-3/2) (2H / R^3) R = -3H / R^2

Summary
-------

a = 2H / R^3
b = -3H / R^2
c = 0
d = H

f(x) = (2H / R^3) x^3 + (-3H / R^2) x^2 + H
= H [2 (x / R)^3 - 3 (x / R)^2 + 1]

Bumpy quartic
=============

Problem
-------

f(x) = ax^4 + bx^2 + cx + d
f'(x) = 4ax^3 + 2bx + c

Solve a, b, c, d from
f(0) = H
f(R) = 0
f'(0) = 0
f'(R) = 0

Solution
--------

f(0) = d = H
f'(0) = c = 0

f'(R) = 4aR^3 + 2bR = 0
=>
b = -2aR^2

f(R) = aR^4 + bR^2 + H
= aR^4 - 2aR^4 + H
= -aR^4 + H = 0

=>
a = H / R^4

=>
b = -2H / R^2

Summary
-------

a = H / R^4
b = -2H / R^2
c = 0
d = H

g(x) = (x^2 / R^2)^2 - 2 (x^2 / R^2) + 1

f(x) = H g(x)

Two-dimensional
---------------

Let d(x, y) = sqrt(x^2 + y^2)
g(x, y) = f(d(x, y))
= H [((x^2 + y^2) / R^2)^2 - 2 (x^2 + y^2) / R^2 + 1]

Area of bumpy quartic
=====================

A = 2 int[0..R] H [(x / R)^4 - 2 (x / R)^2 + 1] dx
= 2HR int[0..1] x^4 - 2 x^2 + 1 dx
= 2HR [0..1] (1/5)x^5 - (2/3)x^3 + x
= 2HR (1/5 - 2/3 + 1)
= 2HR (3 - 10 + 15) / 15
= 2HR (8 / 15)
= HR (16 / 15)

=>

H = (15 / 16)A / R

Finite support rational quadratic function
==========================================

Problem
-------

Let

f(x) = (1 / (ax^2 + bx + c)) + d,      for 0 < x < R
     = 0,                              otherwise
     
f'(x) = -(2ax + b) / (ax^2 + bx + c)^2

Solve for a, b, c, d:

f(0) = oo
f(R) = 0
f'(R) = 0
for all x in ]0, R[: f'(x) < 0

Solution
--------

f(0) = oo
=> c = 0

f'(R) = -(2aR + b) / (aR^2 + bR + c)^2 = 0
=> 2aR + b = 0
=> b = -2aR

f(R) = (1 / (aR^2 + bR)) + d
= (1 / (aR^2 - 2aR^2)) + d
= -1 / aR^2 + d
= 0

=>
d = 1 / aR^2

f'(x) < 0
<=>
-(2ax + b) / (ax^2 + bx + c)^2 < 0
<=>
2ax + b > 0
<=>
2ax - 2aR > 0
<=>
a(x - R) > 0
<=>
a < 0

Summary
-------

a = arbitrary negative number
b = -2aR
c = 0
d = 1 / aR^2

f(x) = (1 / (ax^2 - 2aRx)) + 1 / aR^2
= (1 / aR^2) [(1 / ((x / R)^2 - 2(x / R))) + 1]

Finite support rational quartic function
========================================

Problem
-------

Let
f(x) = 1 / (ax^4 + bx^2 + c) + d,        for 0 < x < R
     = 0,                                otherwise

f'(x) = -(4ax^3 + 2bx) / (ax^4 + bx^2 + c)^2

Solve for a, b, c, d:

f(0) = oo
f(R) = 0
f'(R) = 0
for all x in ]0, R[: f'(x) < 0

Solution
--------

f(0) = oo
=>
c = 0

f'(R) = 0
=>
4aR^3 + 2bR = 0
=>
b = -4aR^3 / 2R
= -2aR^2

f(R) = 1 / (aR^4 - 2aR^2 R^2) + d 
= 1 / -aR^4 + d
= -1 / aR^4 + d = 0

=>
d = 1 / aR^4

f'(x) < 0
<=>
-(4ax^3 + 2bx) < 0
<=>
4ax^3 + 2bx > 0
<=>
4ax^3 - 4aR^2x > 0
<=>
ax^3 - aR^2x > 0
<=>
a(x^3 - R^2x) > 0
<=>
a < 0

Because

x^3 - R^2x < 0
<=>
x^2 - R^2 < 0
<=>
x^2 < R^2
<=>
|x| < R
<=>
true

Summary
-------

a = arbitrary negative number
b = -2aR^2
c = 0
d = 1 / aR^4

f(x) = 1 / (ax^4 + bx^2 + c) + d
= 1 / (ax^4 - 2aR^2 x^2) + 2 / aR^4
= (1 / aR^4) [(1 / [(x / R)^4 - 2(x / R)^2]) + 1]

Diagonal <=> Triangular + normal
================================

Claim
-----

M is diagonal
<=>
M is triangular and normal

'=>'
----

A diagonal matrix is triangular.
Furthermore, because diagonal
matrices commute,
M*M = MM*. Thus M is normal.

'<='
----

Thanks to Angus Rodgers for this one
(he credits the book "Linear algebra done right").

Assume M is upper triangular.

We will use the following
normality condition:

for all v: |Mv| = |M*v|

If v = e1, then
|Mv|^2 = m*(1, 1)m(1, 1)
On the other hand
|M*v|^2 = sum[i = 1..n] m*(1, 1)m(1, 1) + ... + m*(1, n)m(1, n)

Because of normality m(1, 2) = 0, ..., m(1, n) = 0.

Now repeat a similar deduction for e2, using the results
for e1. The end results is that M is diagonal.
Clearly the same deduction can be carried away with
lower triangular M.

[]

Normal linear operators
=======================

Claim
-----

for all v: |Mv| = |M*v|
<=>
MM* = M*M (M is normal)

'<='
----

Thanks to Robert Isreal for this one.

(for all v:)

|Mv| = |M*v|
<=>
|Mv|^2 = |M*v|^2

|Mv|^2 
= <Mv, Mv>
= <v, M*Mv>
= <v, MM*v>
= <M*v, M*v>
= |M*v|^2

'=>'
----

(for all v:) 

|Mv| = |M*v|
=>
|Mv|^2 = |M*v|^2
=>
<Mv, Mv> = <M*v, M*v>
=>
<M*Mv, v> - <MM*v, v> = 0
=>
<(M*M - MM*)v, v> = 0     (1)

Let 
A = M*M - MM*

Now we have at least two ways to prove the
claim. However, the second assumes less
background than the first. We give both.

Proof 1:

A* = (M*M - MM*)* = M*M - MM* = A
=> A is symmetric
=> A is normal
=> A is diagonalizable by Schur decomposition theorem 
plus the fact that M diagonal <=> M triangular and normal.

A = UDU*

U unitary, D diagonal.

(1)
=>
<Av, v> = 0
=>
<UDU*v, v> = 0
=>
<DU*v, U*v> = 0
=>
<Dv, v> = 0      (2)

Where (2) is deduced as follows.
U* is unitary and thus has full rank. 
Thus U*v spans the same space as v.

Plug in standard basis vectors e1, ..., en to show D(i, i) = 0. Thus 

(2)
=>
D = 0
=>
A = 0 
=> 
M*M - MM* = 0 
=> 
M*M = MM* 
=> 
M is normal.

Proof 2:

Thanks to Angus Rodgers for this one (he credits
the book 'Linear algebra done right').

(for all u and v:)

4<Au, v> = <A(u + w),  u + w>  - <A(u - w),  u - w>  +
           <A(u + iw), u + iw> - <A(u - iw), u - iw>

so, from <Av, v> = 0 for all v, we get <Au, v> = 0 for all u and v,
and in particular ||Au||^2 = <Au, Au> = 0, so Au = 0 for all u, i.e. 
A = 0, i.e. MM* = M*M.

[ ]

Least squares linear transformation
===================================

Given two vector sets P = {p_i}, Q = {q_i}
in R^d with n vectors in each. Find the "best"
linear transformation that approximately maps each p_i to q_i.

Solution by Rob Johnson:

Compute

C = sum_i (p_i - p') (p_i - p')^T
D = sum_i (q_i - q') (p_i - p')^T

Then

A = D C^-1

Least squares affine transformation
===================================

Given two point sets P = {p_i}, Q = {q_i}
in R^d with n points in each. Find the "best"
affine transformation that approximately maps each p_i to q_i.

Notation
--------

P = [p_1, ..., p_n]
Q = [q_1, ..., q_n]

Formal problem statement
------------------------

Let

A in R^(n x n)
b in R^n

f(x; A, b) = Ax + b

Find A and b such that the squared error

err(A, b) = sum_i |q_i - f(p_i; A, b)|^2 

is minimized

Solution
--------

Let

err(A, b)
= sum_i |q_i - f(p_i; A, b)|^2 
= sum_i |q_i - Ap_i - b|^2
= sum_i |(q_i - b) - Ap_i|^2
= sum_i ((q_i - b) - Ap_i)^T ((q_i - b) - Ap_i)
= sum_i (q_i - b)^T (q_i - b) - 2p_i^T A^T (q_i - b) + p_i^T A^T A p_i

derr / db_j = sum_i 2(b_j - q_i_j + p_i^T Aj^T) = sum_i 2((Aj p_i + b_j) - q_i_j)
=>
derr / db = sum_i 2((A p_i + b) - q_i)

derr / db = 0
=>
sum_i 2((A p_i + b) - q_i) = 0
=>
sum_i (A p_i + b) - q_i = 0
=>
A [sum_i p_i] + n b - [sum_i q_i] = 0
=>
A [sum_i p_i]/n + b - [sum_i q_i]/n = 0

Let 
P' = [sum_i p_i] / n
Q' = [sum_i q_i] / n

Then
A P' + b = Q'

That is, the best affine transformation maps the centroid
of P to the centroid of Q.

Translate the point sets P and Q by their centroids
and find the least squares linear transformation between
these sets to find A.

Then

b = Q' - A P'

Least squares similarity transformation in 2D
=============================================

Solution by Rob Johnson:

p' = centroid of p_i
q' = centroid of q_i

S = (P - P')^T (Q - Q')

Compute the polar decomposition of S:

S = UD 

with U orthonormal and D symmetric positive semidefinite.

r1 = sum_i dot(p_i - p')
r2 = sum_i dot((q - q')U, p - p')

r = r1 / r2

The similarity transformation is then given by:

f(x) = rUx + (q' - rUp')

If you can't compute the polar decomposition, then
compute the svd:

S = A B C^T = (A C^T) (C B C^T)
=>
U = A C^T

Invertible skew-symmetric matrices
==================================

Let A be an n x n matrix
and n odd.

Claim:

There are no invertible skew-symmetric
matrices in odd dimensions.

Proof:

A^T = -A
=>
|A^T| = |-A|
=>
|A| = (-1)^n |A|
=>
(-1)^n = 1 or |A| = 0
=>
n is even or A is singular

[]

Perpendicular by a linear transformation
========================================

A direct implication of the Hairy ball theorem is
the following: in R^n, for odd n, there is no
continuous function that maps all non-zero vectors
to their non-zero (arbitrary) perpendiculars.

Let us prove this statement for the case
of linear continuous functions. Such a function
is of the form

f : R^n -> R^n: f(x) = Ax

with A an n x n matrix.

Proof 1:
(Thanks to "Niels Diepeveen" from sci.math)

An n x n matrix has n eigenvalues. If n is odd,
then there must be at least one real eigenvalue,
because complex eigenvalues come in conjugate pairs.
Thus there is an eigenvector v such that:

f(v) = Av = tv

with t the real eigenvalue. Certainly v is not
perpendicular to tv.

[]

Proof 2:
(Thanks to "Hagman" from sci.math)

Sketch:
1) A must be invertible
2) A must be skew-symmetric
3) There are no invertible skew-symmetric
matrices in odd dimensions.

If two vectors are not collinear then
they don't have a common perpendicular. Also,
if two vectors are collinear and differ by a 
non-unity scalar, then by linearity the perpendiculars
differ by the same scalar.
Thus f is injective. But then f must also be surjective
and thus bijective. Thus A is invertible.

Perpendicularity production of f implies:

for all x in R^n: x^T A x = 0

<=>
for all x in R^n: x^T A^T x = 0

<=> 
for all x in R^n: x^T (A + A^T) x = 0 (1)

Let S = A + A^T

S is symmetric and so can be decomposed as

S = Q^T D Q

with Q orthogonal and D diagonal.

(1) 
<=>
for all x in R^n: x^T Q^T D Q x = 0

<=> (columns of Q form a basis)
for all x in R^n: x^T D x = 0

<=> (substitute standard basis vectors for x)
D = 0

<=>
S = 0

<=>
A + A^T = 0

<=>
A^T = -A

<=>
(A is skew-symmetric)

But there are no invertible skew-symmetric matrices for odd n.

[]

Fourier transformation of the box filter
========================================

Let
f(t) = {1, for -0.5 <= t <= 0.5
       {0, otherwise

F(f) = int[-oo..oo] e^(-2pi * ift) f(t) dt
= int[-0.5..0.5] e^(-2pi * ift) dt
= (1 / (-2pi * if)) * (e^(-2pi * if * 0.5) - e^(2pi * if * 0.5))

e^(-ib) - e^(ib) = (cos(-b) + i sin(-b)) - (cos(b) + i sin(b))
= -2i sin(b)

=>
F(f) = (1 / (-2pi * if)) * (-2i sin(pi * f))
= sin(pi * f) / (pi * f)
= sinc(f)

Active pixels
=============

Problem:

A (one-dimensional) "pixel" is an element of {[x, x + 1[ in R | x in Z}.

Given an interval A = [x1, x2[ in R, a pixel [x, x + 1[ is called "active"
if (x + 0.5) is contained in A.

Identify pixels by their starting points (integers). 
Given a real interval, give the set of active pixels (an integer interval).

Solution:

We wish to find a function f: P(R) -> P(Z)
that maps half-open intervals in the reals to
half open intervals in the integers, such
that the integer interval contains all the active pixels in
the real interval.
We consider the problem in two steps by decomposing f to
two functions:

f([x1, x2[) = [f1(x1), f2(x2)[

Consider f1. We require:
f1(x) = 0, if x e [0, 0.5]
f1(x) = 1, if x e ]0.5, 1[
f1(x + n) = f1(x) + n, n in Z

It can be seen the following function satisfies us:
f1(x) = ceil(x - 0.5).
For example:
f1(0.4) = ceil(0.4 - 0.5) = ceil(-0.1) = 0
f1(0.5) = ceil(0.5 - 0.5) = ceil(0) = 0
f1(0.6) = ceil(0.6 - 0.5) = ceil(0.1) = 1
and
f1(x + n) = ceil(x + n - 0.5) = ceil(x - 0.5) + n = f1(x) + n

Now consider f2. We require:
f2(x) = 0, if x e [0, 0.5]
f2(x) = 1, if x e ]0.5, 1[
f2(x + n) = f2(x) + n, n in Z

But that's the same as f1. So:

f([x1, x2[) = [ceil(x1 - 0.5), ceil(x2 - 0.5)[

Sinc integrates to 1
====================

Let
sinc(x) = sin(pi * x) / (pi * x), when x != 0 
          1, otherwise

Claim:

integrate[-oo..oo](sinc(x) dx) = 1

Proof:

A lame proof by using the sine integral follows.

Si[t] = integrate[0..t]((sin(x) / x) dx)

Si[oo] = pi / 2 
(Taken from Mathworld. How do you prove it?)

integrate[-oo..oo](sinc(x) dx)
= 2 * integrate[0..oo](sinc(x) dx)
= 2 * integrate[0..oo](sinc(u / pi) * (1 / pi) dx)
= 2 * (1 / pi) * integrate[0..oo](sin(u) / u dx)
= (2 / pi) * Si[oo]
= 1

Sinc filter has no frequency ripple
===================================

Thanks to 'rancid moth' from sci.math for help with this.

Let
sinc(x) = sin(pi * x) / (pi * x), when x != 0 
          1, otherwise

Claim:

for all x in [0, 1[ in R:
S = sum[i = -oo..oo](sinc(x + i)) = 1

Proof:

Assume x != 0.

S = sum[i = -oo..oo](sin(pi * (x + i)) / (pi * (x + i)))
= (sin(pi * x) / pi) * sum[i = -oo..oo]((-1)^i / (x + i))

Let
U = sum[i = -oo..oo]((-1)^i / (x + i))
= (1 / x) + sum[i = 1..oo]((-1)^i / (x + i) + (-1)^(-i) / (x - i))
= (1 / x) + sum[i = 1..oo]((-1)^i / (x + i) + (-1)^(-i) / (x - i))
= (1 / x) + sum[i = 1..oo]((-1)^i * ((x - i) + (x + i)) / (x^2 - i^2))
= (1 / x) + 2 * x * sum[i = 1..oo]((-1)^i / (x^2 - i^2))

U / pi = (1 / (pi * x)) + 2 * (pi * x) * sum[i = 1..oo]((-1)^i / ((pi * x)^2 - (pi * i)^2))
= csc(pi * x) = 1 / sin(pi * x)

=>
U = pi / sin(pi * x)

=>
S = (sin(pi * x) / pi) * U = 1

Assume x = 0.

Then 
S = sum[i = -oo..oo](sinc(x + i)) = sinc(0) = 1

[]

Midpoint minimizes squared error
================================

Let P = {p[i]} be a set of vectors in R^m, with |P| = n. 
Let f(x) = sum(i) ||p[i] - x||^2
with ||x|| the 2-norm.

Claim:

f has its minimum at x' = sum(i)[p[i]] / n

Proof:

f(x)
= sum(i)[dot(p[i] - x)]
= sum(i)[dot(p[i]) - 2 * dot(p[i], x) + dot(x, x)]

df(x) / dx[j]
= sum(i)[
0 -
2 * p[i][j] +
2 * x[j]]
= -2 * sum(i)[p[i][j]] + 2 * n * x[j]

df(x) / dx[j] = 0
=>
x[j] = sum(i)[p[i][j]] / n

The previous works for any j, so:

x = sum(i)[p[i]] / n

d^2f(x) / dx[j]^2 = 2 * n > 0
=> x is a minimum

[]

