Simplex noise derivation
========================

[Parent]: noise.txt

Abstract
------

The simplex noise algorithm, as described in the original paper (see references), 
has remained a bit mystical because the paper does not provide any derivations 
of the mathematical results. In addition, the provided source 
code is optimized to the point of being very hard to understand. To address 
these issues, Stefan Gustavson has written a paper to explain the algorithm 
more intuitively with source code that is easy to understand (see references). 
However, neither of the papers give a detailed derivation of the mathematical
results in n-dimensional space. Such derivations are important in
convincing others for the correctness of the algorithm as well as to establishing 
claimed properties. Indeed, we shall show that the original algorithm contains 
several problems, some of which are caused by errors in derivations, and some of 
which are fundamental. Specifically, in this paper we provide the following:

 * A derivation of the important results used in this algorithm in 
 n-dimensional space.
 
 * Observe that the scale of features is not invariant to dimensionality.
 
 * Give a solution to the above problem by uniform scaling.
 
 * Observe that with increasing dimension the algorithm generates 
 increasing amounts of zero value. In the limit the space contains
 nothing else than the zero value.

 * Observe that the original algorithm is still exponential in
 complexity, although claimed otherwise.

Theory
------

### Identifying simplices with vertex sets

Let ''P = {p_0, p_1, ..., p_n} sub RR^n'', where the points of ''P''
are affinely independent. Then the convex hull of ''P'' is a simplex. 
The convex hull of a k-sized subset of ''P'' is a k-simplex.
In what follows, we identify a simplex with its vertices.
A simplex is called _regular_ if it holds that

''\exists d in RR: \forall i, j in [0, n]: i != j => |p_i - p_j| = d''

### Non-regular simplicial partitioning

In the following, by a partitioning of ''RR^n'' we mean a collection of sets
which cover ''RR^n'' and whose pair-wise intersection has Lebesgue measure zero.
The simplex noise algorithm assumes that ''RR^n'' has been partitioned by 
regular simplices. To find such a partitioning, first consider a partioning by cubes of the form 
''C(i_1, ..., i_n) = [i_1, i_1 + 1] xx ... xx [i_n, i_n + 1] sub RR^n''. The vertices of ''C(i_1, ..., i_n)'' 
are clearly given by ''V(i_1, ..., i_n) = {i_1, i_1 + 1} xx ... xx {i_n, i_n + 1} sub RR^n''.
We would like to partition each cube with simplices such that each simplex contains as vertices
''[i_1, ..., i_n]'' and ''[i_1 + 1, ..., i_n + 1]''. Each such simplex can be identified with a 
traversal from ''[i_1, ..., i_n]'' to ''[i_1 + 1, ..., i_n + 1]'' using ''e_i'' as displacements,
where ''e_i'' is the i:th standard basis vector. Clearly there are ''n!'' number of such traversals,
each specifying ''(n + 1)'' vertices of a simplex.

This procedure gives a partitioning of ''RR^n'' into simplices. However, these simplices are not regular:
while each edge on the traversal path is of length 1, the edge from the last vertex to the first is
of length ''sqrt(n)'' (for example). 

### Scaling along a cube diagonal

A simple transformation that turns these simplices regular
is to scale the space along the vector ''p = [1, ..., 1]''. Such a scaling is given by:

''f : RR^n -> RR^n: f(x) = x + (alpha - 1) (: x, p :) / (: p, p :) p = x + (alpha - 1) / n (sum_{i = 1}^n x_i) p''

where ''alpha in RR'', ''alpha > 0'', and ''(: ., . :)'' is the dot product. 
Applying this scaling to the standard basis vectors ''e_i'' gives:

''f(e_i) = e_i + (alpha - 1) / n p''

We would now like to determine ''alpha'' such that 
''{0, f(e_1), ... sum_{i = 1}^k f(e_i), ..., sum_{i = 1}^n f(e_i)}'' is a simplex.
The squared length of ''f(e_i)'' is given by:

''(: f(e_i) :) = ((alpha - 1) / n + 1)^2 + (n - 1)((alpha - 1) / n)^2''

On the other hand:

''sum_{i = 1}^n f(e_i) = (alpha - 1) p + p = alpha p''

and

''(: sum_{i = 1}^n f(e_i) :) = alpha^2 n

We require the squared lengths to coincide:

''alpha^2 n = ((alpha - 1) / n + 1)^2 + (n - 1)((alpha - 1) / n)^2''

This can be solved as:

''alpha = +- 1 / sqrt(n + 1)''

Of which we select (the other one changes orientation):

''alpha = 1 / sqrt(n + 1)''

It can then be shown that all edges of the simplex are of equal length ''d = sqrt(n / (n + 1))''.

### Deforming transformation

The transformation from the non-regular simplex partitioning to
a regular simplex partitioning is linear and is given by the matrix ''M in RR^{n xx n}'':

''M = [f(e_1), ..., f(e_n)] = (1 / sqrt(n + 1) - 1) / n [p, ..., p] + I = 
(1 - sqrt(n + 1)) / (n sqrt(n + 1)) [p, ..., p] + I''

It can be shown that the inverse matrix of ''M'' is given by:

''M^{-1} = (sqrt(n + 1) - 1) / n [p, ..., p] + I''

These are the transformations that are used in the original algorithm. 
However, the problem with these transformations is that the simplex edge length
varies with dimension. This in turn implies that the feature size varies with
dimension. Therefore, it is better to apply an additional uniform scaling, which
expands distances but does not affect angles. Since the length of the simplex
edges is ''d'', the edge length of 1 is obtained by multiplying
with ''1 / d'':

''B = 1 / d M''

''B^{-1} = d M^{-1}''

### Containing simplex

Assume the space is partitioned according to the regular simplicial
partitioning as described above. Given a point ''u in RR^n'', 
we would like to obtain the vertices of that simplex which contains ''u''.
 
This is easily solved by transforming the vertices of the
simplices to the integer grid. This works because containment is invariant 
to linear transforms. This transform was already given as the matrix ''B^{-1}''.
It is worthwhile to expand the matrix-vector multiplication ''B^{-1} u'' because
of the simple structure of the matrix:

''x = [x_1, ..., x_n]^T = 
B^{-1} [u_1, ..., u_n]^T = 
d * ([u_1, ..., u_n]^T + (s * sum_{i = 1}^n u_i) * [1, ..., 1]^T)''

where ''s = (sqrt(n + 1) - 1) / n''. The simplicity of this transformation is not a coincidence:
the simplex partitioning was specifically chosen (by Ken Perlin) to yield this kind of transformation.
The original algorithm does not have the ''d'' factor.

### Vertices of the containing simplex

An integer cube is a set of the form ''[h_1, ..., h_n] xx ... xx [h_1 + 1, ..., h_n + 1] sub RR^n'',
where ''[h_1, ..., h_n] in ZZ^n''. The integer cubes can be identified by their lower integer 
coordinates ''[h_1, ..., h_n] in ZZ^n''. Clearly the containing cube for a point ''x'' is given by

''h = [h_1, ..., h_n]^T = text(floor)([x_1, ..., x_n]^T) = text(floor)(x)''.

The simplices inside the cube can be identified with traversals in ''{h_1, h_1 + 1} xx ... xx {h_n, h_n + 1} sub ZZ^n'',
such that the traversal begins from ''[h_1, ..., h_n]'' and ends to ''[h_1 + 1, ..., h_n + 1]'',
where one is only allowed to move along standard basis axes and no backing is allowed. It is easily
seen that the number of different traversals is ''n!'', and that the path contains exactly
''n + 1'' vertices.

Let ''b = x - h'', ''b in \[0, 1\[^n sub RR^n''. Further, order the components of ''b'' into
descending order ''{b_{r_1}, ..., b_{r_n}}''. Then the vertices of the containing simplex are
given by ''{h, h + e_{r_1}, ..., h + sum_{i = 1}^k e_{r_i}, ..., h + sum_{i = 1}^n e_{r_i}}''.
These (n + 1) vertices can be mapped back to the regular simplex space by using the transformation ''B''.

### Scalar fields 

Consider the regular simplex space again. Each simplex vertex in this space is
associated with a gradient vector. This is the gradient of the noise
function at that point. The noise function itself is zero at each vertex.
The gradient vector ''g in RR^n'' of a given vertex ''v in RR^n'' defines a 
scalar field ''s'' centered on ''v'' by:

''s : RR^n -> RR : s(x) = (: x, g :)''

This scalar field is attenuated by a spherically symmetric function ''a''
centered on ''v'':

''a : RR^n -> RR : a(x) = {((1 - (: x :) / (r^2 d^2))^4, text(, when ) (: x :) <= r^2 d^2),
                                                     (0, text(, otherwise))]''

where ''r in \]0, 1\] sub RR'' is the percentage of the simplex edge length ''d'' after which
''a'' must have been attenuated to zero. Because of the normalization of the simplex edge
length, we have ''d = 1'' and thus the expression for ''a'' simplifies to:

''a : RR^n -> RR : a(x) = {((1 - (: x :) / r^2)^4, text(, when ) (: x :) <= r^2),
                                               (0, text(, otherwise))]''

Note that this attenuation function is a bit different from the one given by Perlin in his paper.
The original function has problems with scale-invariance (this is related to the fact that there
is no normalization of the simplex edge length in the original algorithm: ''d'' changes with
dimension but this is not taken into account anywhere).

The product of the scalar functions ''s'' and ''a'', centered on ''v'', is given by:

''e: RR^n -> RR : e(x) = s(x) * a(x)''

### Choosing ''r''

The ''r'' must be chosen appropriately: the support of the function ''e'' must be
such that it is contained in the union of those simplices which are connected
to the given vertex. If this were not the case, then it would not be sufficient
to evaluate ''(n + 1)'' vertices for a given point. Out of these values we want 
the largest one. This is given by ''r'' such that ''r d = h'', where ''h'' is the 
height of a regular simplex of edge length ''d'':

''h = d sqrt((3 / 4)^{n - 1})''

Thus

''r = sqrt((3 / 4)^{n - 1})''

and

''r^2 = (3 / 4)^{n - 1}''

However, from these formulas we see that as dimension grows, more and more
of the scalar field ''e'' inside the simplex is zero. For example, in ''RR^7'',
only 42% of the edge length of a simplex is covered by the scalar field of its 
either end-vertex. This means that 16% of the edge has ''e'' zero. Moreover, 
a remarkable amount of volume inside the simplex has ''e'' zero.
This shows that the reconstruction method in simplex noise does not generalize
appropriately for higher dimensions.

### Noise function

Let ''V = {v_1, v_2, ...} sub RR^n'' be the set of all simplex vertices. The noise function is 
given by:

''text(noise)(x) : RR^n -> RR : text(noise)(x) = sum_{i = 1}^oo e(x - v_i)

Because ''e'' has finite support, this sum actually contains only ''(n + 1)'' non-zero terms:
these terms are those corresponding to the vertices of the simplex containing ''x''.

### Normalization constant to have noise range in ''\[-1, 1\]''

It is important to normalize the output of the noise function such that it has certain
minimum and maximum values. We choose these as -1 and 1, respectively. However,
I don't know how to approach this problem.

References
----------

_Real-Time Shading SIGGRAPH Course Notes_, Chapter 2: Noise Hardware, 
Ken Perlin, 2001.

_Simplex Noise Demystified_, Stefan Gustavson, 2005.

